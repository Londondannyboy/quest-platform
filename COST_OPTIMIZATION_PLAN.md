# Cost Optimization Implementation Plan
## 54% Research Cost Reduction ($325/month savings)

**Date:** October 10, 2025
**Based on:** Comprehensive cost analysis by Claude Desktop
**Target:** Reduce research costs from $0.45 ‚Üí $0.15 per article (average)
**Implementation Time:** 1 week

---

## üìä Current Problem

**Cost per article: $0.61**
```
Research (6 APIs):    $0.45  ‚Üê 75% of total cost!
  ‚îú‚îÄ Perplexity:      $0.15  (33% of article cost)
  ‚îú‚îÄ DataForSEO:      $0.10  (22%)
  ‚îú‚îÄ Tavily:          $0.05  (11%)
  ‚îú‚îÄ Serper:          $0.05  (11%)
  ‚îú‚îÄ LinkUp:          $0.05  (11%)
  ‚îî‚îÄ Firecrawl:       $0.05  (11%)
Content (Haiku):      $0.03
Embeddings:           $0.01
Images (4√ó FLUX):     $0.12
```

**Root Cause:**
Calling Perplexity ($0.15) for EVERY article when 70% of articles are in the same topic cluster

---

## üí° The Solution: Cluster Research Reuse

### Concept
Instead of researching every article individually, research **once per topic cluster** and reuse for 10-50 articles:

**Current (wasteful):**
```python
# 50 articles about "Portugal Digital Nomad"
for article in articles:
    research = await perplexity.research(article.topic)  # $0.45 √ó 50 = $22.50
    content = await claude.generate(research)
```

**Optimized (smart):**
```python
# Research cluster ONCE
cluster_research = await perplexity.research("Portugal Digital Nomad")  # $0.45 once!

# Generate 50 articles from same research
for article in articles:
    content = await claude.generate(cluster_research)  # FREE research!
    # Only cost: DataForSEO keyword validation ($0.10 per article)
```

**Savings:** $22.50 ‚Üí $5.45 (76% reduction for this cluster)

---

## üéØ Three-Tier Research Strategy

### Tier 1: High Priority (Perplexity)
**Cost:** $0.10 (DataForSEO) + $0.15 (Perplexity) = $0.25
**Use for:** High-traffic keywords, YMYL content, flagship articles
**Examples:** "Portugal Digital Nomad Visa", "Spain Golden Visa"

### Tier 2: Medium Priority (Tavily)
**Cost:** $0.10 (DataForSEO) + $0.05 (Tavily) = $0.15
**Use for:** Mid-tier keywords, general information, comparison content
**Examples:** "Cost of Living Lisbon", "Best Coworking Spaces"

### Tier 3: Low Priority (Haiku Synthesis)
**Cost:** $0.10 (DataForSEO) + $0.01 (Haiku) = $0.11
**Use for:** Low-traffic keywords, lifestyle content, cultural topics
**Examples:** "Portuguese Culture", "Language Learning Tips"

### Tier 4: Cluster Reuse (FREE!)
**Cost:** $0.10 (DataForSEO only)
**Use for:** 70% of articles (same cluster as recent research)
**Savings:** $0.45 saved per article

---

## üìà Cost Projection

### Current (1000 articles/month)
```
1000 articles √ó $0.45 research = $450/month
```

### Optimized (with cluster reuse + tiered routing)
```
Scenario A: Reuse cluster (70% = 700 articles)
700 √ó $0.10 (DataForSEO only) = $70

Scenario B: Medium priority (20% = 200 articles)
200 √ó $0.15 (DataForSEO + Tavily) = $30

Scenario C: High priority (10% = 100 articles)
100 √ó $0.25 (DataForSEO + Perplexity) = $25

Total: $70 + $30 + $25 = $125/month
Savings: $450 - $125 = $325/month (72% reduction!)
Annual savings: $3,900/year
```

---

## üóÑÔ∏è Database Schema (Created)

### `topic_clusters` Table
**Purpose:** Organize topics into strategic clusters

```sql
CREATE TABLE topic_clusters (
    id SERIAL PRIMARY KEY,
    name VARCHAR(200),           -- "Portugal Digital Nomad"
    priority VARCHAR(20),         -- high, medium, low
    research_tier VARCHAR(20),    -- perplexity, tavily, haiku
    primary_keywords TEXT[],
    article_count INTEGER DEFAULT 0,
    total_research_cost DECIMAL(10,2)
);
```

### `cluster_research` Table
**Purpose:** Cache research for cluster reuse (90-day TTL)

```sql
CREATE TABLE cluster_research (
    id SERIAL PRIMARY KEY,
    cluster_id INTEGER REFERENCES topic_clusters(id),
    research_data JSONB,          -- Cached research
    seo_data JSONB,              -- DataForSEO results
    serp_analysis JSONB,         -- Competitor analysis
    ai_insights JSONB,           -- Perplexity/Tavily/Haiku
    ai_provider VARCHAR(50),     -- Which API used
    research_cost DECIMAL(10,2),
    reuse_count INTEGER DEFAULT 0,
    expires_at TIMESTAMPTZ        -- 90-day cache
);
```

**Status:** ‚úÖ Migration file created: `004_cluster_research.sql`

---

## üîß Implementation Files (Created)

### 1. ResearchGovernance Class ‚úÖ
**File:** `backend/app/core/research_governance.py`
**Purpose:** Route research decisions intelligently

**Key Methods:**
```python
async def get_research_decision(topic: str) -> (ResearchDecision, TopicCluster):
    """
    Returns:
    - REUSE_CLUSTER: Use cached research (FREE!)
    - RESEARCH_PERPLEXITY: High priority ($0.15)
    - RESEARCH_TAVILY: Medium priority ($0.05)
    - RESEARCH_HAIKU: Low priority ($0.01)
    - SKIP: Not in strategy
    """

async def find_cluster(topic: str) -> TopicCluster:
    """Smart keyword matching to find cluster"""

async def has_recent_research(cluster_id: int) -> bool:
    """Check 90-day cache"""

async def store_cluster_research(...):
    """Save research for reuse"""

async def get_cost_savings_stats() -> Dict:
    """Track savings from reuse"""
```

### 2. Enhanced ResearchAgent (Pending)
**File:** `backend/app/agents/research.py` (needs modification)

**Changes needed:**
```python
class ResearchAgent:
    def __init__(self):
        self.governance = ResearchGovernance()  # NEW
        # ... existing code

    async def run(self, topic: str) -> Dict:
        # 1. Check governance FIRST
        decision, cluster = await self.governance.get_research_decision(topic)

        if decision == ResearchDecision.REUSE_CLUSTER:
            logger.info("research.reused", cost_saved=0.45)
            return cluster.research_data  # FREE!

        # 2. DataForSEO foundation (ALWAYS)
        seo_data = await self.dataforseo.analyze(topic)
        cost = Decimal("0.10")

        # 3. Conditional AI research (based on tier)
        if decision == ResearchDecision.RESEARCH_PERPLEXITY:
            ai_research = await self.perplexity.research(topic)
            cost += Decimal("0.15")
        elif decision == ResearchDecision.RESEARCH_TAVILY:
            ai_research = await self.tavily.research(topic)
            cost += Decimal("0.05")
        else:  # RESEARCH_HAIKU
            ai_research = await self.haiku.synthesize(seo_data)
            cost += Decimal("0.01")

        # 4. Store for cluster reuse
        await self.governance.store_cluster_research(
            cluster.id, research, seo_data, ai_research, ...
        )

        return research
```

---

## üöÄ Implementation Steps

### Week 1: Database + Governance (Completed)

**Day 1-2: Database Schema** ‚úÖ
- [x] Create `004_cluster_research.sql` migration
- [x] Define `topic_clusters` table
- [x] Define `cluster_research` table
- [x] Seed initial clusters from QUEST_RELOCATION_RESEARCH.md
- [ ] Run migration on Neon database

**Day 3-4: Research Governance** ‚úÖ
- [x] Create `ResearchGovernance` class
- [x] Implement cluster detection
- [x] Implement reuse logic
- [x] Implement cost tracking
- [ ] Add monitoring views

**Day 5: Integration**
- [ ] Modify `ResearchAgent` to use governance
- [ ] Add cluster_id to article creation
- [ ] Test end-to-end flow

**Day 6-7: Testing & Validation**
- [ ] Generate 10 test articles in same cluster
- [ ] Verify research reuse working
- [ ] Validate cost savings
- [ ] Monitor cache hit rate

---

## üìä Success Metrics

### Before Optimization
- Research cost per article: $0.45
- Monthly research cost (1000 articles): $450
- Cache hit rate: 40% (vector similarity)
- Duplicate research: Common

### After Optimization (Target)
- Research cost per article: $0.15 (average)
- Monthly research cost (1000 articles): $125
- Cache hit rate: 70% (cluster reuse)
- Duplicate research: Eliminated

### Monitoring Queries
```sql
-- Cost savings dashboard
SELECT * FROM cluster_cost_analysis
ORDER BY cost_saved DESC;

-- Research reuse stats
SELECT * FROM research_reuse_stats
WHERE research_date > CURRENT_DATE - INTERVAL '7 days';

-- Cluster performance
SELECT
    name,
    article_count,
    total_research_cost,
    ROUND(total_research_cost / NULLIF(article_count, 0), 2) as cost_per_article
FROM topic_clusters
WHERE article_count > 0
ORDER BY article_count DESC;
```

---

## üéØ Expected Results

### Immediate (Week 1)
- ‚úÖ Database schema deployed
- ‚úÖ ResearchGovernance operational
- ‚è≥ First cluster articles using reuse

### Short-term (Month 1)
- 40-50% cost reduction as clusters build up
- 10-15 active clusters with research
- 3-5 articles per cluster average

### Long-term (Month 3)
- 70% cost reduction at scale
- 30-50 active clusters
- 10-20 articles per cluster
- **$325/month saved** = $3,900/year

---

## ‚ö†Ô∏è Important Notes

### Token Limits - NO CHANGES NEEDED
- `max_tokens=8192` is CORRECT (Claude's actual limit)
- "Minimum 2000 words" is guidance, not a hard limit
- Haiku/Sonnet will generate appropriate length based on research depth

### Research Quality Maintained
- High-priority clusters still get Perplexity research
- SERP analysis and competitor scraping unchanged
- DataForSEO keyword validation on EVERY article
- Only reusing broader cluster research, not keyword-specific data

### Cache Management
- 90-day TTL for cluster research
- Manual "mark stale" for breaking news
- Automatic expiry on date-sensitive topics

---

## üèÅ Next Actions

### Immediate (Today)
1. ‚úÖ Review and validate optimization plan
2. ‚è≥ Run database migration (`004_cluster_research.sql`)
3. ‚è≥ Integrate ResearchGovernance into ResearchAgent

### This Week
4. Test with first cluster (Portugal Digital Nomad)
5. Generate 10 articles using cluster reuse
6. Validate cost savings
7. Document results

### Next Week
8. Roll out to all clusters
9. Monitor cost reduction
10. Optimize cluster definitions based on usage

---

**Implementation Status:** Database + Governance code complete
**Pending:** Migration execution + ResearchAgent integration
**Estimated Time to Production:** 1 week
**Estimated Annual Savings:** $3,900
